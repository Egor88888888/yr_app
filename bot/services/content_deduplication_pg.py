"""
üîç –°–ò–°–¢–ï–ú–ê –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê - PostgreSQL –≤–µ—Ä—Å–∏—è
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç 100% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–æ–≤ –∏ —Ç–µ–º –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PostgreSQL —á–µ—Ä–µ–∑ SQLAlchemy –≤–º–µ—Å—Ç–æ SQLite
"""

import hashlib
import re
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass
from difflib import SequenceMatcher

from sqlalchemy import select, and_, or_
from sqlalchemy.ext.asyncio import AsyncSession
from .db import async_sessionmaker, ContentFingerprint

logger = logging.getLogger(__name__)

@dataclass
class ContentFingerprintData:
    """–û—Ç–ø–µ—á–∞—Ç–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    title_hash: str
    content_hash: str
    topic_keywords: Set[str]
    semantic_tokens: Set[str]
    legal_references: Set[str]
    content_type: str
    full_text_hash: str


class PostgreSQLContentDeduplicationSystem:
    """–°–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å PostgreSQL"""
    
    def __init__(self):
        self.similarity_threshold = 0.7
        self.keyword_overlap_threshold = 0.6
        self.legal_ref_weight = 0.9
        logger.info("üîç PostgreSQL Content Deduplication System initialized")
    
    def _extract_keywords(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return set()
        
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        clean_text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = clean_text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–ø—Ä–∏', '–∑–∞', '–±–µ–∑', '–ø–æ–¥', '–Ω–∞–¥', '–æ–±', '–æ', '–ø—Ä–æ', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–≤–º–µ—Å—Ç–æ'}
        keywords = {word for word in words if len(word) > 3 and word not in stop_words}
        
        return keywords
    
    def _extract_legal_references(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ (—Å—Ç–∞—Ç—å–∏, –∑–∞–∫–æ–Ω—ã, –∫–æ–¥–µ–∫—Å—ã)"""
        if not text:
            return set()
        
        legal_patterns = [
            r'—Å—Ç(?:–∞—Ç—å—è)?\.?\s*\d+(?:\.\d+)*',  # –°—Ç–∞—Ç—å—è 123, —Å—Ç. 456.1
            r'–ø(?:—É–Ω–∫—Ç)?\.?\s*\d+(?:\.\d+)*',   # –ü—É–Ω–∫—Ç 1, –ø. 2.3
            r'—á(?:–∞—Å—Ç—å)?\.?\s*\d+(?:\.\d+)*',   # –ß–∞—Å—Ç—å 1, —á. 2
            r'[–ê-–Ø][–∞-—è]+\s+–∫–æ–¥–µ–∫—Å',            # –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å
            r'–§–ó\s*‚Ññ?\s*\d+',                   # –§–ó ‚Ññ123
            r'–∑–∞–∫–æ–Ω\s+‚Ññ?\s*\d+',                # –ó–∞–∫–æ–Ω ‚Ññ456
        ]
        
        references = set()
        for pattern in legal_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            references.update(match.strip() for match in matches)
        
        return references
    
    def _create_content_hash(self, text: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ö–µ—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        if not text:
            return ""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        return hashlib.md5(normalized.encode('utf-8')).hexdigest()
    
    def create_fingerprint(self, title: str, content: str, content_type: str = "post") -> ContentFingerprintData:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–ø–µ—á–∞—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        full_text = f"{title} {content}"
        
        return ContentFingerprintData(
            title_hash=self._create_content_hash(title),
            content_hash=self._create_content_hash(content),
            full_text_hash=self._create_content_hash(full_text),
            topic_keywords=self._extract_keywords(full_text),
            semantic_tokens=self._extract_keywords(content),
            legal_references=self._extract_legal_references(full_text),
            content_type=content_type
        )
    
    def _calculate_similarity(self, fp1: ContentFingerprintData, fp2: ContentFingerprintData) -> float:
        """–†–∞—Å—á–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –º–µ–∂–¥—É –æ—Ç–ø–µ—á–∞—Ç–∫–∞–º–∏"""
        if fp1.full_text_hash == fp2.full_text_hash:
            return 1.0
        
        # –°—Ö–æ–¥—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords1, keywords2 = fp1.topic_keywords, fp2.topic_keywords
        if not keywords1 and not keywords2:
            keyword_similarity = 0.0
        elif not keywords1 or not keywords2:
            keyword_similarity = 0.0
        else:
            intersection = len(keywords1 & keywords2)
            union = len(keywords1 | keywords2)
            keyword_similarity = intersection / union if union > 0 else 0.0
        
        # –°—Ö–æ–¥—Å—Ç–≤–æ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
        legal1, legal2 = fp1.legal_references, fp2.legal_references
        if legal1 and legal2:
            legal_intersection = len(legal1 & legal2)
            legal_union = len(legal1 | legal2)
            legal_similarity = legal_intersection / legal_union if legal_union > 0 else 0.0
        else:
            legal_similarity = 0.0
        
        # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        total_similarity = (
            keyword_similarity * 0.6 +
            legal_similarity * self.legal_ref_weight * 0.4
        )
        
        return min(total_similarity, 1.0)
    
    async def validate_and_register_content(
        self, 
        title: str, 
        content: str, 
        content_type: str = "post",
        block_duration_hours: int = 24
    ) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        Returns: (is_unique, reason)
        """
        try:
            fingerprint = self.create_fingerprint(title, content, content_type)
            
            async with async_sessionmaker() as session:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏
                exact_duplicate = await session.execute(
                    select(ContentFingerprint)
                    .where(ContentFingerprint.full_text_hash == fingerprint.full_text_hash)
                )
                
                if exact_duplicate.scalar_one_or_none():
                    return False, "–¢–æ—á–Ω–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –¥—É–±–ª–∏
                similar_content = await session.execute(
                    select(ContentFingerprint)
                    .where(
                        and_(
                            ContentFingerprint.content_type == content_type,
                            or_(
                                ContentFingerprint.blocked_until.is_(None),
                                ContentFingerprint.blocked_until < datetime.now()
                            )
                        )
                    )
                )
                
                for existing in similar_content.scalars():
                    existing_fp = ContentFingerprintData(
                        title_hash=existing.title_hash,
                        content_hash=existing.content_hash,
                        full_text_hash=existing.full_text_hash,
                        topic_keywords=set(json.loads(existing.topic_keywords or "[]")),
                        semantic_tokens=set(json.loads(existing.semantic_tokens or "[]")),
                        legal_references=set(json.loads(existing.legal_references or "[]")),
                        content_type=existing.content_type
                    )
                    
                    similarity = self._calculate_similarity(fingerprint, existing_fp)
                    if similarity >= self.similarity_threshold:
                        return False, f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (—Å—Ö–æ–¥—Å—Ç–≤–æ: {similarity:.1%})"
                
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                block_until = datetime.now() + timedelta(hours=block_duration_hours)
                
                new_fingerprint = ContentFingerprint(
                    title_hash=fingerprint.title_hash,
                    content_hash=fingerprint.content_hash,
                    full_text_hash=fingerprint.full_text_hash,
                    content_type=content_type,
                    topic_keywords=json.dumps(list(fingerprint.topic_keywords)),
                    semantic_tokens=json.dumps(list(fingerprint.semantic_tokens)),
                    legal_references=json.dumps(list(fingerprint.legal_references)),
                    blocked_until=block_until
                )
                
                session.add(new_fingerprint)
                await session.commit()
                
                logger.info(f"‚úÖ Content registered: {content_type} - {title[:50]}...")
                return True, "–ö–æ–Ω—Ç–µ–Ω—Ç —É–Ω–∏–∫–∞–ª–µ–Ω"
                
        except Exception as e:
            logger.error(f"‚ùå Content validation error: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ —Ä–∞–∑—Ä–µ—à–∞–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é (fail-open)
            return True, f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}"
    
    async def get_blocked_topics_count(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º"""
        try:
            async with async_sessionmaker() as session:
                result = await session.execute(
                    select(ContentFingerprint)
                    .where(ContentFingerprint.blocked_until > datetime.now())
                )
                return len(result.scalars().all())
        except Exception:
            return 0
    
    async def cleanup_old_fingerprints(self, days_old: int = 90):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –æ—Ç–ø–µ—á–∞—Ç–∫–æ–≤"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_old)
            
            async with async_sessionmaker() as session:
                old_fingerprints = await session.execute(
                    select(ContentFingerprint)
                    .where(ContentFingerprint.created_at < cutoff_date)
                )
                
                count = 0
                for fp in old_fingerprints.scalars():
                    await session.delete(fp)
                    count += 1
                
                await session.commit()
                logger.info(f"üßπ Cleaned up {count} old fingerprints")
                return count
                
        except Exception as e:
            logger.error(f"‚ùå Cleanup error: {e}")
            return 0


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã
_pg_deduplication_system = None

def get_pg_deduplication_system() -> PostgreSQLContentDeduplicationSystem:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ PostgreSQL —Å–∏—Å—Ç–µ–º—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    global _pg_deduplication_system
    if _pg_deduplication_system is None:
        _pg_deduplication_system = PostgreSQLContentDeduplicationSystem()
    return _pg_deduplication_system


async def validate_and_register_content(title: str, content: str, content_type: str = "post") -> Tuple[bool, str]:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    system = get_pg_deduplication_system()
    return await system.validate_and_register_content(title, content, content_type)