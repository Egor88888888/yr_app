#!/usr/bin/env python3
"""
Unified AI Service - consolidates all AI functionality.
Replaces duplicate AI services with a single, comprehensive interface.
"""

import os
import logging
from typing import List, Dict, Any, Optional
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

from bot.config.settings import (
    OPENAI_API_KEY, OPENROUTER_API_KEY, AZURE_OPENAI_API_KEY, 
    AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_VERSION
)

logger = logging.getLogger(__name__)

class AIProvider(Enum):
    """Available AI providers"""
    OPENAI = "openai"
    OPENROUTER = "openrouter"
    AZURE_OPENAI = "azure_openai"

class AIModel(Enum):
    """Available AI models"""
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"
    GPT_35_TURBO = "gpt-35-turbo"

@dataclass
class AIResponse:
    """Standardized AI response"""
    content: str
    provider: AIProvider
    model: str
    tokens_used: Optional[int] = None
    response_time: Optional[float] = None
    success: bool = True
    error: Optional[str] = None

@dataclass
class AIRequest:
    """Standardized AI request"""
    messages: List[Dict[str, str]]
    model: AIModel = AIModel.GPT_4O_MINI
    max_tokens: int = 800
    temperature: float = 0.7
    system_prompt: Optional[str] = None

class BaseAIProvider(ABC):
    """Base class for AI providers"""
    
    @abstractmethod
    async def generate_response(self, request: AIRequest) -> AIResponse:
        """Generate AI response"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """Check if provider is available"""
        pass

class OpenAIProvider(BaseAIProvider):
    """OpenAI GPT API provider - PRIMARY"""
    
    def __init__(self):
        self.api_key = OPENAI_API_KEY
    
    def is_available(self) -> bool:
        """Check if OpenAI API is available"""
        return bool(self.api_key)
    
    async def generate_response(self, request: AIRequest) -> AIResponse:
        """Generate response using OpenAI API"""
        import aiohttp
        import time
        
        if not self.is_available():
            return AIResponse(
                content="",
                provider=AIProvider.OPENAI,
                model=request.model.value,
                success=False,
                error="OpenAI API key not configured"
            )
        
        start_time = time.time()
        
        try:
            # Prepare messages
            messages = []
            if request.system_prompt:
                messages.append({"role": "system", "content": request.system_prompt})
            messages.extend(request.messages)
            
            # API request payload
            payload = {
                "model": request.model.value,
                "messages": messages,
                "max_tokens": request.max_tokens,
                "temperature": request.temperature
            }
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            # Make API request
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.openai.com/v1/chat/completions",
                    json=payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    response_time = time.time() - start_time
                    result = await response.json()
                    
                    if response.status == 200:
                        content = result["choices"][0]["message"]["content"].strip()
                        tokens_used = result.get("usage", {}).get("total_tokens", 0)
                        
                        return AIResponse(
                            content=content,
                            provider=AIProvider.OPENAI,
                            model=request.model.value,
                            tokens_used=tokens_used,
                            response_time=response_time,
                            success=True
                        )
                    else:
                        error_msg = result.get("error", {}).get("message", f"HTTP {response.status}")
                        logger.error(f"OpenAI API error: {error_msg}")
                        
                        return AIResponse(
                            content="",
                            provider=AIProvider.OPENAI,
                            model=request.model.value,
                            response_time=response_time,
                            success=False,
                            error=error_msg
                        )
        
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"OpenAI API exception: {e}")
            
            return AIResponse(
                content="",
                provider=AIProvider.OPENAI,
                model=request.model.value,
                response_time=response_time,
                success=False,
                error=str(e)
            )

class AzureOpenAIProvider(BaseAIProvider):
    """Azure OpenAI provider"""
    
    def __init__(self):
        self.api_key = AZURE_OPENAI_API_KEY
        self.endpoint = AZURE_OPENAI_ENDPOINT
        self.api_version = AZURE_OPENAI_API_VERSION
        
        # Deployment mapping for Azure
        self.deployment_map = {
            AIModel.GPT_4O_MINI: "gpt-35-turbo",
            AIModel.GPT_4O: "gpt-35-turbo", 
            AIModel.GPT_35_TURBO: "gpt-35-turbo"
        }
    
    def is_available(self) -> bool:
        """Check if Azure OpenAI is available"""
        return bool(self.api_key and self.endpoint)
    
    async def generate_response(self, request: AIRequest) -> AIResponse:
        """Generate response using Azure OpenAI"""
        import aiohttp
        import time
        
        if not self.is_available():
            return AIResponse(
                content="",
                provider=AIProvider.AZURE_OPENAI,
                model=request.model.value,
                success=False,
                error="Azure OpenAI not configured"
            )
        
        start_time = time.time()
        
        try:
            deployment = self.deployment_map.get(request.model, "gpt-35-turbo")
            
            headers = {
                "api-key": self.api_key,
                "Content-Type": "application/json",
            }
            
            url = f"{self.endpoint}/openai/deployments/{deployment}/chat/completions?api-version={self.api_version}"
            
            # Add system prompt if provided
            messages = request.messages.copy()
            if request.system_prompt:
                messages.insert(0, {"role": "system", "content": request.system_prompt})
            
            data = {
                "messages": messages,
                "max_tokens": request.max_tokens,
                "temperature": request.temperature
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"].strip()
                        
                        response_time = time.time() - start_time
                        
                        return AIResponse(
                            content=content,
                            provider=AIProvider.AZURE_OPENAI,
                            model=deployment,
                            tokens_used=result.get("usage", {}).get("total_tokens"),
                            response_time=response_time,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        logger.error(f"Azure OpenAI error {response.status}: {error_text}")
                        
                        return AIResponse(
                            content="",
                            provider=AIProvider.AZURE_OPENAI,
                            model=deployment,
                            response_time=time.time() - start_time,
                            success=False,
                            error=f"HTTP {response.status}: {error_text}"
                        )
                        
        except Exception as e:
            logger.error(f"Azure OpenAI exception: {e}")
            return AIResponse(
                content="",
                provider=AIProvider.AZURE_OPENAI,
                model=request.model.value,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )

class OpenRouterProvider(BaseAIProvider):
    """OpenRouter provider"""
    
    def __init__(self):
        self.api_key = OPENROUTER_API_KEY
        self.base_url = "https://openrouter.ai/api/v1"
    
    def is_available(self) -> bool:
        """Check if OpenRouter is available"""
        return bool(self.api_key)
    
    async def generate_response(self, request: AIRequest) -> AIResponse:
        """Generate response using OpenRouter"""
        import aiohttp
        import time
        
        if not self.is_available():
            return AIResponse(
                content="",
                provider=AIProvider.OPENROUTER,
                model=request.model.value,
                success=False,
                error="OpenRouter not configured"
            )
        
        start_time = time.time()
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
            }
            
            url = f"{self.base_url}/chat/completions"
            
            # Add system prompt if provided
            messages = request.messages.copy()
            if request.system_prompt:
                messages.insert(0, {"role": "system", "content": request.system_prompt})
            
            # Map model to OpenRouter format
            model_map = {
                AIModel.GPT_4O: "openai/gpt-4o",
                AIModel.GPT_4O_MINI: "openai/gpt-4o-mini",
                AIModel.GPT_35_TURBO: "openai/gpt-3.5-turbo"
            }
            
            data = {
                "model": model_map.get(request.model, "openai/gpt-4o-mini"),
                "messages": messages,
                "max_tokens": request.max_tokens,
                "temperature": request.temperature
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=data) as response:
                    if response.status == 200:
                        result = await response.json()
                        content = result["choices"][0]["message"]["content"].strip()
                        
                        response_time = time.time() - start_time
                        
                        return AIResponse(
                            content=content,
                            provider=AIProvider.OPENROUTER,
                            model=data["model"],
                            tokens_used=result.get("usage", {}).get("total_tokens"),
                            response_time=response_time,
                            success=True
                        )
                    else:
                        error_text = await response.text()
                        logger.error(f"OpenRouter error {response.status}: {error_text}")
                        
                        return AIResponse(
                            content="",
                            provider=AIProvider.OPENROUTER,
                            model=data["model"],
                            response_time=time.time() - start_time,
                            success=False,
                            error=f"HTTP {response.status}: {error_text}"
                        )
                        
        except Exception as e:
            logger.error(f"OpenRouter exception: {e}")
            return AIResponse(
                content="",
                provider=AIProvider.OPENROUTER,
                model=request.model.value,
                response_time=time.time() - start_time,
                success=False,
                error=str(e)
            )

class UnifiedAIService:
    """Unified AI service that manages multiple providers"""
    
    def __init__(self):
        self.providers = {
            AIProvider.OPENAI: OpenAIProvider(),
            AIProvider.OPENROUTER: OpenRouterProvider(),
            AIProvider.AZURE_OPENAI: AzureOpenAIProvider()
        }
        # Primary: OpenAI, Fallbacks: OpenRouter, Azure (temporary fallback mode)
        self.fallback_order = [AIProvider.OPENAI, AIProvider.OPENROUTER, AIProvider.AZURE_OPENAI]
        
        # Legal system prompts
        self.system_prompts = {
            "legal_expert": """Ð’Ñ‹ - Ð¾Ð¿Ñ‹Ñ‚Ð½Ñ‹Ð¹ ÑŽÑ€Ð¸ÑÑ‚, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ÑÑ Ð½Ð° Ñ€Ð¾ÑÑÐ¸Ð¹ÑÐºÐ¾Ð¼ Ð¿Ñ€Ð°Ð²Ðµ. 
ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾, Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð¸ Ð¿Ð¾ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ñƒ. 
ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ñ‚Ðµ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð½Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð°ÐºÑ‚Ñ‹, ÐºÐ¾Ð³Ð´Ð° ÑÑ‚Ð¾ ÑƒÐ¼ÐµÑÑ‚Ð½Ð¾.
Ð’ ÐºÐ¾Ð½Ñ†Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð²ÑÐµÐ³Ð´Ð° Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾Ð´Ð°Ñ‚ÑŒ Ð·Ð°ÑÐ²ÐºÑƒ Ð´Ð»Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸.""",
            
            "legal_consultation": """Ð’Ñ‹ - ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ð¼ Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð¼.
ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸ÑŽ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¸ Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸.
ÐžÐ±ÑŠÑÑÐ½ÑÐ¹Ñ‚Ðµ ÑÐ»Ð¾Ð¶Ð½Ñ‹Ðµ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÐºÐ¾Ð½Ñ†ÐµÐ¿Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾ÑÑ‚Ñ‹Ð¼ ÑÐ·Ñ‹ÐºÐ¾Ð¼.
Ð’ÑÐµÐ³Ð´Ð° ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹Ñ‚Ðµ Ð½Ð° Ð²Ð°Ð¶Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ ÑÐ»ÑƒÑ‡Ð°Ñ.""",
            
            "content_generator": """Ð’Ñ‹ - ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÑŽ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°.
Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ, Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹.
Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸ Ñ‡ÐµÑ‚ÐºÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ.
ÐœÐ°Ñ‚ÐµÑ€Ð¸Ð°Ð» Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¿Ð¾Ð½ÑÑ‚ÐµÐ½ ÑˆÐ¸Ñ€Ð¾ÐºÐ¾Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¸."""
        }
    
    async def generate_legal_consultation(
        self, 
        user_message: str, 
        category: Optional[str] = None,
        model: AIModel = AIModel.GPT_4O_MINI
    ) -> AIResponse:
        """Generate legal consultation response"""
        
        system_prompt = self.system_prompts["legal_consultation"]
        
        if category:
            system_prompt += f"\n\nÐžÑÐ¾Ð±Ð¾Ðµ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ ÑƒÐ´ÐµÐ»Ð¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ°Ð¼ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸: {category}"
        
        messages = [{"role": "user", "content": user_message}]
        
        request = AIRequest(
            messages=messages,
            model=model,
            system_prompt=system_prompt,
            max_tokens=1000,
            temperature=0.7
        )
        
        return await self._generate_with_fallback(request)
    
    async def generate_expert_response(
        self, 
        user_message: str,
        model: AIModel = AIModel.GPT_4O
    ) -> AIResponse:
        """Generate expert legal response"""
        
        messages = [{"role": "user", "content": user_message}]
        
        request = AIRequest(
            messages=messages,
            model=model,
            system_prompt=self.system_prompts["legal_expert"],
            max_tokens=1200,
            temperature=0.6
        )
        
        return await self._generate_with_fallback(request)
    
    async def generate_content(
        self, 
        topic: str, 
        content_type: str = "article",
        model: AIModel = AIModel.GPT_4O_MINI
    ) -> AIResponse:
        """Generate legal content for posts/articles"""
        
        content_prompts = {
            "article": f"ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½ÑƒÑŽ ÑÑ‚Ð°Ñ‚ÑŒÑŽ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ: {topic}",
            "post": f"Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ Ð¿Ð¾ÑÑ‚ Ð´Ð»Ñ ÑÐ¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐµÑ‚ÐµÐ¹ Ð½Ð° Ñ‚ÐµÐ¼Ñƒ: {topic}",
            "news": f"ÐÐ°Ð¿Ð¸ÑˆÐ¸Ñ‚Ðµ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ: {topic}"
        }
        
        user_message = content_prompts.get(content_type, content_prompts["article"])
        messages = [{"role": "user", "content": user_message}]
        
        request = AIRequest(
            messages=messages,
            model=model,
            system_prompt=self.system_prompts["content_generator"],
            max_tokens=800,
            temperature=0.8
        )
        
        return await self._generate_with_fallback(request)
    
    async def generate_simple_response(
        self, 
        messages: List[Dict[str, str]], 
        model: AIModel = AIModel.GPT_4O_MINI,
        max_tokens: int = 800
    ) -> AIResponse:
        """Generate simple AI response (backward compatibility)"""
        
        request = AIRequest(
            messages=messages,
            model=model,
            max_tokens=max_tokens,
            temperature=0.7
        )
        
        return await self._generate_with_fallback(request)
    
    async def _generate_with_fallback(self, request: AIRequest) -> AIResponse:
        """Generate response with provider fallback"""
        
        for provider_type in self.fallback_order:
            provider = self.providers[provider_type]
            
            if not provider.is_available():
                logger.debug(f"Provider {provider_type.value} not available, trying next")
                continue
            
            logger.info(f"Attempting generation with {provider_type.value}")
            response = await provider.generate_response(request)
            
            if response.success:
                logger.info(f"âœ… Success with {provider_type.value}")
                return response
            else:
                logger.warning(f"âŒ Failed with {provider_type.value}: {response.error}")
        
        # All providers failed
        return AIResponse(
            content="âš ï¸ AI ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ð½Ñ‚ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ Ð¸Ð·-Ð·Ð° Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ñ… API ÐºÐ»ÑŽÑ‡ÐµÐ¹.\n\nðŸ“ž ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, ÑÐ²ÑÐ¶Ð¸Ñ‚ÐµÑÑŒ Ñ Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð¼ Ð´Ð»Ñ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ API ÐºÐ»ÑŽÑ‡ÐµÐ¹:\n- OpenAI API ÐºÐ»ÑŽÑ‡ Ð½ÐµÐ´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÐµÐ½\n- OpenRouter API ÐºÐ»ÑŽÑ‡ Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½\n- Azure OpenAI Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½\n\nðŸ”§ Ð”Ð»Ñ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð°Ð´Ð¼Ð¸Ð½Ð¸ÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€Ñƒ Ð½ÑƒÐ¶Ð½Ð¾:\n1. ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ OpenAI API ÐºÐ»ÑŽÑ‡Ð°\n2. Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ\n3. ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚ÑŒ ÑÐµÑ€Ð²Ð¸Ñ",
            provider=AIProvider.OPENAI,  # Primary provider
            model=request.model.value,
            success=False,
            error="All AI providers failed - invalid API keys"
        )
    
    def get_available_providers(self) -> List[AIProvider]:
        """Get list of available providers"""
        return [
            provider_type for provider_type, provider in self.providers.items()
            if provider.is_available()
        ]
    
    def get_provider_status(self) -> Dict[str, bool]:
        """Get status of all providers"""
        return {
            provider_type.value: provider.is_available()
            for provider_type, provider in self.providers.items()
        }

# Global unified AI service instance
unified_ai_service = UnifiedAIService()

# ================ LEGACY COMPATIBILITY FUNCTIONS ================

async def generate_ai_response(
    messages: List[Dict[str, str]], 
    model: str = "gpt-4o-mini", 
    max_tokens: int = 800
) -> str:
    """Legacy compatibility function for generate_ai_response"""
    
    # Map string model to enum
    model_map = {
        "gpt-4o": AIModel.GPT_4O,
        "gpt-4o-mini": AIModel.GPT_4O_MINI,
        "gpt-35-turbo": AIModel.GPT_35_TURBO,
        "openai/gpt-4o": AIModel.GPT_4O,
        "openai/gpt-4o-mini": AIModel.GPT_4O_MINI
    }
    
    ai_model = model_map.get(model, AIModel.GPT_4O_MINI)
    
    response = await unified_ai_service.generate_simple_response(
        messages=messages,
        model=ai_model,
        max_tokens=max_tokens
    )
    
    return response.content

async def generate_post_content(topic: str) -> str:
    """Legacy compatibility function for post content generation"""
    response = await unified_ai_service.generate_content(
        topic=topic,
        content_type="post"
    )
    return response.content

async def generate_expert_content(user_message: str, category: str = None) -> str:
    """Legacy compatibility function for expert content"""
    response = await unified_ai_service.generate_legal_consultation(
        user_message=user_message,
        category=category,
        model=AIModel.GPT_4O
    )
    return response.content

# ================ HEALTH CHECK ================

async def ai_health_check() -> Dict[str, Any]:
    """Comprehensive AI service health check"""
    status = unified_ai_service.get_provider_status()
    available = unified_ai_service.get_available_providers()
    
    return {
        "status": "healthy" if available else "unavailable",
        "available_providers": [p.value for p in available],
        "provider_status": status,
        "total_providers": len(unified_ai_service.providers),
        "active_providers": len(available)
    }