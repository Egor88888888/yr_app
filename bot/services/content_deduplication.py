"""
üîç –°–ò–°–¢–ï–ú–ê –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–Ø –î–£–ë–õ–ò–†–û–í–ê–ù–ò–Ø –ö–û–ù–¢–ï–ù–¢–ê
–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç 100% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–æ–≤ –∏ —Ç–µ–º –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥–∞
"""

import hashlib
import re
import sqlite3
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set, Tuple
from dataclasses import dataclass
from difflib import SequenceMatcher
import json

logger = logging.getLogger(__name__)

@dataclass
class ContentFingerprint:
    """–û—Ç–ø–µ—á–∞—Ç–æ–∫ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    title_hash: str
    content_hash: str
    topic_keywords: Set[str]
    semantic_tokens: Set[str]
    legal_references: Set[str]
    content_type: str
    created_at: datetime
    full_text_hash: str


class ContentDeduplicationSystem:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, db_path: str = "content_deduplication.db"):
        self.db_path = db_path
        self.similarity_threshold = 0.7  # –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        self.keyword_overlap_threshold = 0.6  # –ü–æ—Ä–æ–≥ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        self.legal_ref_weight = 0.9  # –í–µ—Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫
        
        self._init_database()
        logger.info("üîç Content Deduplication System initialized")
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –æ—Ç–ø–µ—á–∞—Ç–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS content_fingerprints (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title_hash TEXT NOT NULL,
                content_hash TEXT NOT NULL,
                full_text_hash TEXT UNIQUE NOT NULL,
                topic_keywords TEXT NOT NULL,
                semantic_tokens TEXT NOT NULL, 
                legal_references TEXT NOT NULL,
                content_type TEXT NOT NULL,
                source_system TEXT NOT NULL,
                title TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                usage_count INTEGER DEFAULT 1,
                last_used TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS blocked_topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                topic_normalized TEXT UNIQUE NOT NULL,
                topic_original TEXT NOT NULL,
                block_reason TEXT NOT NULL,
                blocked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                blocked_until TIMESTAMP,
                usage_count INTEGER DEFAULT 1
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –≥—Ä—É–ø–ø
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS semantic_groups (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                group_name TEXT NOT NULL,
                keywords TEXT NOT NULL,
                legal_refs TEXT NOT NULL,
                content_count INTEGER DEFAULT 1,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_full_text_hash ON content_fingerprints(full_text_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_content_type ON content_fingerprints(content_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_created_at ON content_fingerprints(created_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_topic_normalized ON blocked_topics(topic_normalized)')
        
        conn.commit()
        conn.close()
    
    def extract_content_fingerprint(self, title: str, content: str, content_type: str = "post", source_system: str = "unknown") -> ContentFingerprint:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ç–ø–µ—á–∞—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏"""
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        normalized_title = self._normalize_text(title)
        normalized_content = self._normalize_text(content)
        full_text = f"{normalized_title} {normalized_content}"
        
        # –•—ç—à–∏
        title_hash = hashlib.md5(normalized_title.encode()).hexdigest()
        content_hash = hashlib.md5(normalized_content.encode()).hexdigest()
        full_text_hash = hashlib.sha256(full_text.encode()).hexdigest()
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ —Ç–µ–º–∞–º
        topic_keywords = self._extract_topic_keywords(full_text)
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
        semantic_tokens = self._extract_semantic_tokens(full_text)
        
        # –ü—Ä–∞–≤–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏
        legal_references = self._extract_legal_references(full_text)
        
        return ContentFingerprint(
            title_hash=title_hash,
            content_hash=content_hash,
            topic_keywords=topic_keywords,
            semantic_tokens=semantic_tokens,
            legal_references=legal_references,
            content_type=content_type,
            created_at=datetime.now(),
            full_text_hash=full_text_hash
        )
    
    def _normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —ç–º–æ–¥–∑–∏ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        text = re.sub(r'[^\w\s\.\,\!\?\;\:]', ' ', text)
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower().strip()
        return text
    
    def _extract_topic_keywords(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º —Ç–µ–º–∞–º"""
        
        legal_topics = {
            # –°–µ–º–µ–π–Ω–æ–µ –ø—Ä–∞–≤–æ
            '—Å–µ–º–µ–π–Ω–æ–µ': ['—Å–µ–º–µ–π–Ω', '—Ä–∞–∑–≤–æ–¥', '–∞–ª–∏–º–µ–Ω—Ç', '–±—Ä–∞–∫', '—Å—É–ø—Ä—É–≥', '—Ä–µ–±–µ–Ω–æ–∫', '–æ–ø–µ–∫–∞'],
            # –¢—Ä—É–¥–æ–≤–æ–µ –ø—Ä–∞–≤–æ
            '—Ç—Ä—É–¥–æ–≤–æ–µ': ['—Ç—Ä—É–¥', '—Ä–∞–±–æ—Ç', '—É–≤–æ–ª—å–Ω', '–∑–∞—Ä–ø–ª–∞—Ç', '–æ—Ç–ø—É—Å–∫', '–±–æ–ª—å–Ω–∏—á–Ω', '—Å–æ–∫—Ä–∞—â–µ–Ω'],
            # –ñ–∏–ª–∏—â–Ω–æ–µ –ø—Ä–∞–≤–æ
            '–∂–∏–ª–∏—â–Ω–æ–µ': ['–∂–∏–ª', '–∫–≤–∞—Ä—Ç–∏—Ä', '–¥–æ–º', '–∫–æ–º–º—É–Ω–∞–ª—å–Ω', '–∞—Ä–µ–Ω–¥', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω', '—É–∫'],
            # –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–∞–≤–∞
            '–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–æ–µ': ['–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª', '—Ç–æ–≤–∞—Ä', '—É—Å–ª—É–≥', '–≤–æ–∑–≤—Ä–∞—Ç', '–≥–∞—Ä–∞–Ω—Ç', '–∫–∞—á–µ', '–º–∞–≥–∞–∑–∏–Ω'],
            # –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–µ –ø—Ä–∞–≤–æ
            '–∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–æ–µ': ['–¥—Ç–ø', '–∞–≤—Ç–æ', '—Å—Ç—Ä–∞—Ö–æ–≤', '–æ—Å–∞–≥–æ', '–∫–∞—Å–∫–æ', '–≥–∏–±–¥–¥', '—à—Ç—Ä–∞—Ñ'],
            # –ù–∞—Å–ª–µ–¥—Å—Ç–≤–æ
            '–Ω–∞—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–µ': ['–Ω–∞—Å–ª–µ–¥', '–∑–∞–≤–µ—â–∞–Ω', '–Ω–∞—Å–ª–µ–¥–Ω–∏–∫', '–Ω–æ—Ç–∞—Ä–∏—É—Å', '–¥–æ–ª—è'],
            # –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–æ–µ': ['–∞–¥–º–∏–Ω', '—à—Ç—Ä–∞—Ñ', '–∫–æ–∞–ø', '–≥–æ—Å', '—Å–ª—É–∂–±', '–≤–µ–¥–æ–º—Å—Ç–≤'],
            # –ë–∞–Ω–∫–æ–≤—Å–∫–æ–µ –∏ —Ñ–∏–Ω–∞–Ω—Å—ã
            '–±–∞–Ω–∫–æ–≤—Å–∫–æ–µ': ['–±–∞–Ω–∫', '–∫—Ä–µ–¥–∏—Ç', '–∑–∞–π–º', '–¥–µ–ø–æ–∑–∏—Ç', '–ø—Ä–æ—Ü–µ–Ω—Ç', '–¥–æ–ª–≥'],
            # –ó–µ–º–µ–ª—å–Ω–æ–µ –ø—Ä–∞–≤–æ
            '–∑–µ–º–µ–ª—å–Ω–æ–µ': ['–∑–µ–º–µ–ª—å–Ω', '—É—á–∞—Å—Ç–æ–∫', '–¥–∞—á–∞', '–º–µ–∂–µ–≤–∞–Ω', '–∫–∞–¥–∞—Å—Ç—Ä'],
            # –£–≥–æ–ª–æ–≤–Ω–æ–µ –ø—Ä–∞–≤–æ
            '—É–≥–æ–ª–æ–≤–Ω–æ–µ': ['—É–≥–æ–ª–æ–≤–Ω', '–ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω', '—Å—Ç–∞—Ç—å—è', '—Å—É–¥', '—Å–ª–µ–¥—Å—Ç–≤']
        }
        
        found_keywords = set()
        text_lower = text.lower()
        
        for category, keywords in legal_topics.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found_keywords.add(f"{category}:{keyword}")
        
        return found_keywords
    
    def _extract_semantic_tokens(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤"""
        
        # –¢–∏–ø–æ–≤—ã–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏
        situations = [
            '–ø—Ä–∞–≤–∞ –Ω–∞—Ä—É—à–µ–Ω—ã', '–ø–æ–ª—É—á–∏—Ç—å –∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—é', '–ø–æ–¥–∞—Ç—å –∏—Å–∫', '–æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ —Å—É–¥',
            '–Ω–∞—Ä—É—à–µ–Ω–∏–µ –∑–∞–∫–æ–Ω–∞', '–∑–∞—â–∏—Ç–∞ –ø—Ä–∞–≤', '–≤–æ–∑–º–µ—â–µ–Ω–∏–µ —É—â–µ—Ä–±–∞', '—é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –ø–æ–º–æ—â—å',
            '–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —é—Ä–∏—Å—Ç–∞', '–ø—Ä–∞–≤–æ–≤–∞—è –∑–∞—â–∏—Ç–∞', '—Å—É–¥–µ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ', '–ø—Ä–∞–≤–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è'
        ]
        
        semantic_tokens = set()
        text_lower = text.lower()
        
        for situation in situations:
            if situation in text_lower:
                semantic_tokens.add(situation.replace(' ', '_'))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∏–≥—Ä–∞–º–º—ã –∏ —Ç—Ä–∏–≥—Ä–∞–º–º—ã
        words = text_lower.split()
        for i in range(len(words) - 1):
            bigram = f"{words[i]}_{words[i+1]}"
            if len(bigram) > 10:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –±–∏–≥—Ä–∞–º–º—ã
                semantic_tokens.add(f"bigram:{bigram}")
        
        for i in range(len(words) - 2):
            trigram = f"{words[i]}_{words[i+1]}_{words[i+2]}"
            if len(trigram) > 15:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Ç—Ä–∏–≥—Ä–∞–º–º—ã
                semantic_tokens.add(f"trigram:{trigram}")
        
        return semantic_tokens
    
    def _extract_legal_references(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫"""
        
        legal_refs = set()
        
        # –°—Ç–∞—Ç—å–∏ –∫–æ–¥–µ–∫—Å–æ–≤ –∏ –∑–∞–∫–æ–Ω–æ–≤
        patterns = [
            r'—Å—Ç(?:–∞—Ç—å—è|\.)\s*(\d+(?:\.\d+)?)\s*(–≥–∫|—Ç–∫|—É–∫|–∫–æ–∞–ø|—Å–∫|–∂–∫|–Ω–∫)\s*—Ä—Ñ',
            r'—Å—Ç–∞—Ç—å—è\s*(\d+(?:\.\d+)?)\s*(–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ|—Ç—Ä—É–¥–æ–≤–æ–≥–æ|—É–≥–æ–ª–æ–≤–Ω–æ–≥–æ|—Å–µ–º–µ–π–Ω–æ–≥–æ|–∂–∏–ª–∏—â–Ω–æ–≥–æ|–Ω–∞–ª–æ–≥–æ–≤–æ–≥–æ)\s*–∫–æ–¥–µ–∫—Å–∞',
            r'–∑–∞–∫–æ–Ω[–∞-—è\s]*‚Ññ\s*(\d+-—Ñ–∑)',
            r'—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π –∑–∞–∫–æ–Ω –æ—Ç [\d\.]+ ‚Ññ (\d+-—Ñ–∑)',
            r'–ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ.*‚Ññ\s*(\d+)',
            r'–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ.*‚Ññ\s*([\d\-]+)',
        ]
        
        text_lower = text.lower()
        
        for pattern in patterns:
            matches = re.finditer(pattern, text_lower)
            for match in matches:
                legal_refs.add(match.group().strip())
        
        return legal_refs
    
    def is_content_duplicate(self, fingerprint: ContentFingerprint, source_system: str = "unknown") -> Tuple[bool, str, float]:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (is_duplicate, reason, similarity_score)
        """
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ —Ö—ç—à—É
        cursor.execute(
            "SELECT title, source_system FROM content_fingerprints WHERE full_text_hash = ?",
            (fingerprint.full_text_hash,)
        )
        exact_match = cursor.fetchone()
        if exact_match:
            conn.close()
            return True, f"–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ø–æ—Å—Ç–æ–º: {exact_match[0]} (—Å–∏—Å—Ç–µ–º–∞: {exact_match[1]})", 1.0
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        cursor.execute('''
            SELECT id, title, topic_keywords, semantic_tokens, legal_references, source_system,
                   created_at, usage_count
            FROM content_fingerprints 
            WHERE content_type = ? 
            AND created_at > ? 
            ORDER BY created_at DESC 
            LIMIT 100
        ''', (fingerprint.content_type, datetime.now() - timedelta(days=30)))
        
        recent_posts = cursor.fetchall()
        
        for post in recent_posts:
            post_id, title, keywords_json, tokens_json, refs_json, post_system, created_at, usage_count = post
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            try:
                post_keywords = set(json.loads(keywords_json))
                post_tokens = set(json.loads(tokens_json))
                post_refs = set(json.loads(refs_json))
            except:
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarity = self._calculate_similarity(
                fingerprint.topic_keywords, post_keywords,
                fingerprint.semantic_tokens, post_tokens,
                fingerprint.legal_references, post_refs
            )
            
            if similarity > self.similarity_threshold:
                reason = f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ {similarity:.2f} —Å –ø–æ—Å—Ç–æ–º: {title} (—Å–∏—Å—Ç–µ–º–∞: {post_system})"
                conn.close()
                return True, reason, similarity
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º
        topic_blocked, block_reason = self._is_topic_blocked(fingerprint.topic_keywords)
        if topic_blocked:
            conn.close()
            return True, f"–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–º–∞: {block_reason}", 0.9
        
        conn.close()
        return False, "", 0.0
    
    def _calculate_similarity(self, keywords1: Set[str], keywords2: Set[str],
                             tokens1: Set[str], tokens2: Set[str],
                             refs1: Set[str], refs2: Set[str]) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keyword_intersection = len(keywords1.intersection(keywords2))
        keyword_union = len(keywords1.union(keywords2))
        keyword_similarity = keyword_intersection / keyword_union if keyword_union > 0 else 0
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        token_intersection = len(tokens1.intersection(tokens2))
        token_union = len(tokens1.union(tokens2))
        token_similarity = token_intersection / token_union if token_union > 0 else 0
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ (–≤—ã—Å–æ–∫–∏–π –≤–µ—Å)
        ref_intersection = len(refs1.intersection(refs2))
        ref_union = len(refs1.union(refs2))
        ref_similarity = ref_intersection / ref_union if ref_union > 0 else 0
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
        total_similarity = (
            keyword_similarity * 0.4 +
            token_similarity * 0.3 +
            ref_similarity * self.legal_ref_weight * 0.3
        )
        
        return total_similarity
    
    def _is_topic_blocked(self, keywords: Set[str]) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        cursor.execute('''
            SELECT topic_normalized, topic_original, block_reason, blocked_until, usage_count
            FROM blocked_topics
            WHERE blocked_until IS NULL OR blocked_until > ?
        ''', (datetime.now().isoformat(),))
        
        blocked_topics = cursor.fetchall()
        
        for topic_norm, topic_orig, reason, blocked_until, usage_count in blocked_topics:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–µ–º—ã –≤ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç
            topic_words = self._normalize_text(topic_orig).split()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ —Ç–µ–º—ã —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            keyword_text = ' '.join([kw.split(':')[-1] if ':' in kw else kw for kw in keywords])
            
            # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–ª–æ–≤ —Ç–µ–º—ã –≤ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö
            matches_found = any(word in keyword_text for word in topic_words if len(word) > 3)
            
            if matches_found:
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ç–µ–º—ã
                cursor.execute(
                    'UPDATE blocked_topics SET usage_count = usage_count + 1 WHERE topic_normalized = ?',
                    (topic_norm,)
                )
                conn.commit()
                conn.close()
                return True, f"{reason} (–ø–æ–ø—ã—Ç–æ–∫: {usage_count + 1})"
        
        conn.close()
        return False, ""
    
    def register_content(self, fingerprint: ContentFingerprint, title: str, source_system: str = "unknown") -> bool:
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
            keywords_json = json.dumps(list(fingerprint.topic_keywords))
            tokens_json = json.dumps(list(fingerprint.semantic_tokens))
            refs_json = json.dumps(list(fingerprint.legal_references))
            
            cursor.execute('''
                INSERT INTO content_fingerprints
                (title_hash, content_hash, full_text_hash, topic_keywords, semantic_tokens,
                 legal_references, content_type, source_system, title, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                fingerprint.title_hash,
                fingerprint.content_hash,
                fingerprint.full_text_hash,
                keywords_json,
                tokens_json,
                refs_json,
                fingerprint.content_type,
                source_system,
                title,
                fingerprint.created_at.isoformat()
            ))
            
            conn.commit()
            logger.info(f"‚úÖ Registered unique content: {title[:50]}...")
            return True
            
        except sqlite3.IntegrityError as e:
            logger.warning(f"‚ùå Failed to register content (duplicate): {str(e)}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error registering content: {str(e)}")
            return False
        finally:
            conn.close()
    
    def block_topic_temporarily(self, topic: str, reason: str, hours: int = 24):
        """–í—Ä–µ–º–µ–Ω–Ω–∞—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ç–µ–º—ã"""
        
        normalized_topic = self._normalize_text(topic)
        blocked_until = datetime.now() + timedelta(hours=hours)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO blocked_topics
            (topic_normalized, topic_original, block_reason, blocked_until)
            VALUES (?, ?, ?, ?)
        ''', (normalized_topic, topic, reason, blocked_until.isoformat()))
        
        conn.commit()
        conn.close()
        
        logger.info(f"‚ö†Ô∏è Topic blocked for {hours}h: {topic} - {reason}")
    
    def block_topic_permanently(self, topic: str, reason: str):
        """Permanent –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Ç–µ–º—ã"""
        
        normalized_topic = self._normalize_text(topic)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO blocked_topics
            (topic_normalized, topic_original, block_reason, blocked_until)
            VALUES (?, ?, ?, NULL)
        ''', (normalized_topic, topic, reason))
        
        conn.commit()
        conn.close()
        
        logger.info(f"üö´ Topic permanently blocked: {topic} - {reason}")
    
    def get_content_statistics(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        cursor.execute('SELECT COUNT(*) FROM content_fingerprints')
        total_content = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT content_type) FROM content_fingerprints')
        content_types = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT source_system) FROM content_fingerprints')
        source_systems = cursor.fetchone()[0]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏—Å—Ç–µ–º–∞–º
        cursor.execute('''
            SELECT source_system, COUNT(*), MAX(created_at)
            FROM content_fingerprints
            GROUP BY source_system
        ''')
        by_system = cursor.fetchall()
        
        # –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
        cursor.execute('SELECT COUNT(*) FROM blocked_topics WHERE blocked_until IS NULL')
        permanently_blocked = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM blocked_topics WHERE blocked_until > ?', 
                      (datetime.now().isoformat(),))
        temporarily_blocked = cursor.fetchone()[0]
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        cursor.execute('SELECT MAX(created_at) FROM content_fingerprints')
        last_activity = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_registered_content': total_content,
            'content_types': content_types,
            'source_systems': source_systems,
            'by_system': by_system,
            'permanently_blocked_topics': permanently_blocked,
            'temporarily_blocked_topics': temporarily_blocked,
            'last_activity': last_activity,
            'similarity_threshold': self.similarity_threshold
        }
    
    def cleanup_old_data(self, days: int = 90):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        cutoff_date = datetime.now() - timedelta(days=days)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ—Ç–ø–µ—á–∞—Ç–∫–∏
        cursor.execute(
            'DELETE FROM content_fingerprints WHERE created_at < ?',
            (cutoff_date.isoformat(),)
        )
        deleted_fingerprints = cursor.rowcount
        
        # –£–¥–∞–ª—è–µ–º –∏—Å—Ç–µ–∫—à–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        cursor.execute(
            'DELETE FROM blocked_topics WHERE blocked_until IS NOT NULL AND blocked_until < ?',
            (datetime.now().isoformat(),)
        )
        deleted_blocks = cursor.rowcount
        
        conn.commit()
        conn.close()
        
        logger.info(f"üßπ Cleanup: deleted {deleted_fingerprints} fingerprints and {deleted_blocks} expired blocks")
        
        return {
            'deleted_fingerprints': deleted_fingerprints,
            'deleted_blocks': deleted_blocks
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
_deduplication_system = None

def get_deduplication_system() -> ContentDeduplicationSystem:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–∏—Å—Ç–µ–º—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏"""
    global _deduplication_system
    if _deduplication_system is None:
        _deduplication_system = ContentDeduplicationSystem()
    return _deduplication_system


# –£–¥–æ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def check_content_uniqueness(title: str, content: str, content_type: str = "post", 
                           source_system: str = "unknown") -> Tuple[bool, str, float]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (is_unique, reason_if_duplicate, similarity_score)
    """
    system = get_deduplication_system()
    fingerprint = system.extract_content_fingerprint(title, content, content_type, source_system)
    is_duplicate, reason, score = system.is_content_duplicate(fingerprint, source_system)
    return not is_duplicate, reason, score


def register_unique_content(title: str, content: str, content_type: str = "post", 
                          source_system: str = "unknown") -> bool:
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    system = get_deduplication_system()
    fingerprint = system.extract_content_fingerprint(title, content, content_type, source_system)
    return system.register_content(fingerprint, title, source_system)


def validate_and_register_content(title: str, content: str, content_type: str = "post", 
                                source_system: str = "unknown") -> Tuple[bool, str]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –æ–¥–Ω–æ–º –≤—ã–∑–æ–≤–µ
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (success, message)
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    is_unique, reason, score = check_content_uniqueness(title, content, content_type, source_system)
    
    if not is_unique:
        return False, f"–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ —É–Ω–∏–∫–∞–ª–µ–Ω: {reason} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {score:.2f})"
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    registered = register_unique_content(title, content, content_type, source_system)
    
    if registered:
        return True, "–ö–æ–Ω—Ç–µ–Ω—Ç –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É –∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω"
    else:
        return False, "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"