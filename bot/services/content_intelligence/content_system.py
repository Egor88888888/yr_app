"""
üß† MAIN CONTENT INTELLIGENCE SYSTEM
–ì–ª–∞–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–º–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import asyncio
import hashlib
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from sqlalchemy import select, func
from ..db import async_sessionmaker
from ..ai import generate_ai_response
from .models import NewsItem, ContentItem, PostHistory
from .news_parser import NewsParser
from .content_analyzer import ContentAnalyzer
from .post_generator import PostGenerator

logger = logging.getLogger(__name__)

class ContentIntelligenceSystem:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã –∫–æ–Ω—Ç–µ–Ω—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
    
    def __init__(self):
        self.analyzer = ContentAnalyzer()
        self.generator = PostGenerator()
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        logger.info("üß† Initializing Content Intelligence System...")
        logger.info("‚úÖ Content Intelligence System initialized")
    
    async def collect_and_process_news(self) -> List[str]:
        """–°–±–æ—Ä –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π, –≤–æ–∑–≤—Ä–∞—Ç –≥–æ—Ç–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤"""
        posts = []
        
        try:
            # 1. –ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π
            async with NewsParser() as parser:
                news_items = await parser.parse_all_sources()
            
            logger.info(f"üì∞ Collected {len(news_items)} news items")
            
            # 2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö
            relevant_items = []
            for item in news_items:
                relevance = await self.analyzer.analyze_relevance(item)
                if relevance > 0.3:
                    item.relevance_score = relevance
                    relevant_items.append(item)
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unique_items = await self._filter_duplicates(relevant_items)
            logger.info(f"üîç Filtered to {len(unique_items)} unique items")
            
            # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ—Å—Ç–æ–≤
            saved_items = await self._save_items(unique_items)
            
            for item in saved_items[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ—Å—Ç–∞
                try:
                    post_text = await self.generator.generate_post(item)
                    if not await self._is_post_duplicate(post_text):
                        await self._save_post_history(post_text)
                        posts.append(post_text)
                except Exception as e:
                    logger.error(f"Failed to generate post: {e}")
            
            logger.info(f"üìù Generated {len(posts)} posts")
            
        except Exception as e:
            logger.error(f"‚ùå Content processing failed: {e}")
        
        return posts
    
    async def _filter_duplicates(self, items: List[NewsItem]) -> List[NewsItem]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        unique_items = []
        existing_hashes = await self._get_existing_hashes()
        
        for item in items:
            if item.content_hash not in existing_hashes:
                unique_items.append(item)
                existing_hashes.add(item.content_hash)
        
        return unique_items
    
    async def _get_existing_hashes(self) -> set:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö hash –∏–∑ –ë–î"""
        try:
            async with async_sessionmaker() as session:
                cutoff_date = datetime.now() - timedelta(days=30)
                result = await session.execute(
                    select(ContentItem.content_hash)
                    .where(ContentItem.created_at > cutoff_date)
                )
                return {row[0] for row in result.fetchall()}
        except Exception:
            return set()
    
    async def _save_items(self, items: List[NewsItem]) -> List[NewsItem]:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î"""
        saved_items = []
        try:
            async with async_sessionmaker() as session:
                for item in items:
                    db_item = ContentItem(
                        title=item.title,
                        content=item.content,
                        url=item.url,
                        source=item.source,
                        publish_date=item.publish_date,
                        category=item.category,
                        relevance_score=item.relevance_score,
                        content_hash=item.content_hash,
                    )
                    session.add(db_item)
                    saved_items.append(item)
                await session.commit()
        except Exception as e:
            logger.error(f"Failed to save items: {e}")
        return saved_items
    
    async def _is_post_duplicate(self, post_text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å—Ç–æ–≤"""
        try:
            post_hash = hashlib.sha256(post_text.encode()).hexdigest()
            async with async_sessionmaker() as session:
                result = await session.execute(
                    select(PostHistory).where(PostHistory.post_hash == post_hash)
                )
                return result.scalar_one_or_none() is not None
        except Exception:
            return False
    
    async def _save_post_history(self, post_text: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ—Å—Ç–æ–≤"""
        try:
            post_hash = hashlib.sha256(post_text.encode()).hexdigest()
            async with async_sessionmaker() as session:
                post_record = PostHistory(
                    post_text=post_text,
                    post_hash=post_hash,
                    channel_id="auto_generated"
                )
                session.add(post_record)
                await session.commit()
        except Exception as e:
            logger.error(f"Failed to save post history: {e}")
