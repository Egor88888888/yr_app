from __future__ import annotations
# avoid circular? maybe import inside function.

"""Background jobs for analytics and external channel parsing."""

import os
import aiohttp
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
import re
import hashlib

from telethon import functions, types
from telethon.client import TelegramClient
from telegram.ext import ContextTypes
from sqlalchemy import select
from sqlalchemy.ext.asyncio import async_sessionmaker

from db import Subscriber, ExternalPost

# Environment thresholds
VIEWS_THRESHOLD = int(os.getenv("VIEWS_THRESHOLD", 300))  # –°–Ω–∏–∑–∏–ª –ø–æ—Ä–æ–≥
REACTIONS_THRESHOLD = int(os.getenv("REACTIONS_THRESHOLD", 5))  # –°–Ω–∏–∑–∏–ª –ø–æ—Ä–æ–≥
EXTERNAL_CHANNELS: List[str] = [c.strip() for c in os.getenv(
    "EXTERNAL_CHANNELS", "").split(',') if c.strip()]

# RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–ü–ò–°–û–ö
RSS_SOURCES = [
    # === –°—Ç—Ä–∞—Ö–æ–≤—ã–µ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Å–∞–π—Ç—ã ===
    {
        "name": "banki_ru_insurance",
        "url": "https://www.banki.ru/xml/news.rss",
        "category": "insurance"
    },
    {
        "name": "finmarket_ru",
        "url": "https://www.finmarket.ru/rss/news.xml",
        "category": "finance"
    },
    {
        "name": "interfax_finance",
        "url": "https://www.interfax.ru/rss.asp?sec=1208",
        "category": "finance"
    },

    # === –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –∏ –ø—Ä–∞–≤–æ–≤—ã–µ —Ä–µ—Å—É—Ä—Å—ã ===
    {
        "name": "garant_law",
        "url": "https://www.garant.ru/rss/news.xml",
        "category": "legal"
    },
    {
        "name": "pravo_gov_ru",
        "url": "http://pravo.gov.ru/rss.xml",
        "category": "legal"
    },
    {
        "name": "rapsinews_legal",
        "url": "https://rapsinews.ru/rss/",
        "category": "legal"
    },

    # === –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ ===
    {
        "name": "autostat_news",
        "url": "https://www.autostat.ru/rss/news/",
        "category": "auto"
    },
    {
        "name": "autoreview_ru",
        "url": "https://autoreview.ru/rss/all",
        "category": "auto"
    },
    {
        "name": "kolesa_ru",
        "url": "https://www.kolesa.ru/rss",
        "category": "auto"
    },

    # === –î–µ–ª–æ–≤—ã–µ –∏ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Å–∞–π—Ç—ã ===
    {
        "name": "rbc_business",
        "url": "https://rssexport.rbc.ru/rbcnews/news/20/full.rss",
        "category": "business"
    },
    {
        "name": "kommersant_auto",
        "url": "https://www.kommersant.ru/RSS/section-auto.xml",
        "category": "business"
    },
    {
        "name": "ria_news",
        "url": "https://ria.ru/export/rss2/archive/index.xml",
        "category": "news"
    },

    # === –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã ===
    {
        "name": "rsa_autoins",
        "url": "https://www.autoins.ru/ru/rss/news.xml",
        "category": "auto_insurance"
    },
    {
        "name": "cbr_news",
        "url": "https://www.cbr.ru/rss/news/",
        "category": "finance"
    },
    {
        "name": "tass_economics",
        "url": "https://tass.ru/rss/v2.xml?section=51",
        "category": "economics"
    }
]

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê
NEWS_KEYWORDS = [
    # === –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ö–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã ===
    "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö–æ–≤–∞—è", "—Å—Ç—Ä–∞—Ö–æ–≤—â–∏–∫", "—Å—Ç—Ä–∞—Ö–æ–≤–∞—Ç–µ–ª—å", "—Å—Ç—Ä–∞—Ö–æ–≤–æ–π",
    "–≤—ã–ø–ª–∞—Ç–∞", "–≤—ã–ø–ª–∞—Ç—ã", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è", "—É—â–µ—Ä–±",
    "–ø–æ–ª–∏—Å", "—Ñ—Ä–∞–Ω—à–∏–∑–∞", "–ø—Ä–µ–º–∏—è", "—Ç–∞—Ä–∏—Ñ",

    # === –í–∏–¥—ã —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è ===
    "–û–°–ê–ì–û", "–ö–ê–°–ö–û", "–û–°–ì–û–ü", "–î–ú–°", "–û–ú–°",
    "–∞–≤—Ç–æ—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–º–µ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∂–∏–∑–Ω–∏",
    "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∏–º—É—â–µ—Å—Ç–≤–∞", "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏",
    "–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ",

    # === –î–¢–ü –∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è —Ç–µ–º–∞—Ç–∏–∫–∞ ===
    "–î–¢–ü", "–∞–≤–∞—Ä–∏—è", "—Å—Ç–æ–ª–∫–Ω–æ–≤–µ–Ω–∏–µ", "–Ω–∞–µ–∑–¥", "–ø–µ—Ä–µ–≤–æ—Ä–æ—Ç",
    "—Ä–µ–º–æ–Ω—Ç –∞–≤—Ç–æ", "–æ—Ü–µ–Ω–∫–∞ —É—â–µ—Ä–±–∞", "–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞",
    "–∞–≤—Ç–æ—ç–∫—Å–ø–µ—Ä—Ç", "—Ç–µ—Ö–æ—Å–º–æ—Ç—Ä", "–ì–ò–ë–î–î", "–ü–î–î",

    # === –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã ===
    "—Å—É–¥", "–∏—Å–∫", "–∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ", "–ø—Ä–µ—Ç–µ–Ω–∑–∏—è", "–∂–∞–ª–æ–±–∞",
    "–∞—Ä–±–∏—Ç—Ä–∞–∂", "–º–∏—Ä–æ–≤–æ–π —Å—É–¥", "–∞–ø–µ–ª–ª—è—Ü–∏—è", "–∫–∞—Å—Å–∞—Ü–∏—è",
    "—Å—É–¥–µ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ", "–≤–∑—ã—Å–∫–∞–Ω–∏–µ", "–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ª–∏—Å—Ç",

    # === –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ===
    "–†–°–ê", "–¶–ë –†–§", "–†–æ—Å–≥–æ—Å—Å—Ç—Ä–∞—Ö", "–°–û–ì–ê–ó", "–ò–Ω–≥–æ—Å—Å—Ç—Ä–∞—Ö",
    "–ê–ª—å—Ñ–∞–°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "–í–°–ö", "–†–ï–°–û", "—Å—Ç—Ä–∞—Ö–æ–≤–æ–π –æ–º–±—É–¥—Å–º–µ–Ω",
    "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —É–ø–æ–ª–Ω–æ–º–æ—á–µ–Ω–Ω—ã–π", "–§–ê–°", "–†–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä",

    # === –ü—Ä–æ—Ü–µ–¥—É—Ä—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã ===
    "–µ–≤—Ä–æ–ø—Ä–æ—Ç–æ–∫–æ–ª", "–∏–∑–≤–µ—â–µ–Ω–∏–µ –æ –î–¢–ü", "—Å–ø—Ä–∞–≤–∫–∞ –∏–∑ –ì–ò–ë–î–î",
    "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è —Å–ø—Ä–∞–≤–∫–∞", "—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞", "–æ—Ü–µ–Ω–∫–∞",
    "–¥–æ—Å—É–¥–µ–±–Ω–æ–µ —É—Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–ø—Ä—è–º–æ–µ –≤–æ–∑–º–µ—â–µ–Ω–∏–µ —É–±—ã—Ç–∫–æ–≤",
    "—Ä–µ–≥—Ä–µ—Å—Å–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ", "—Å—É–±—Ä–æ–≥–∞—Ü–∏—è",

    # === –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã ===
    "–ª–∏–º–∏—Ç", "–ª–∏–º–∏—Ç –≤–æ–∑–º–µ—â–µ–Ω–∏—è", "—Å—Ç—Ä–∞—Ö–æ–≤–∞—è —Å—É–º–º–∞", "—Å—Ç—Ä–∞—Ö–æ–≤–∞—è –ø—Ä–µ–º–∏—è",
    "—Ç–∞—Ä–∏—Ñ–Ω—ã–π –∫–æ—Ä–∏–¥–æ—Ä", "–±–æ–Ω—É—Å-–º–∞–ª—É—Å", "—Å–∫–∏–¥–∫–∞", "–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç",
    "–ö–ë–ú", "—Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∞–ª—å–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç", "–º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–æ",

    # === –ü—Ä–æ–±–ª–µ–º—ã –∏ —Å–ø–æ—Ä—ã ===
    "–∑–∞–Ω–∏–∂–µ–Ω–∏–µ –≤—ã–ø–ª–∞—Ç", "–æ—Ç–∫–∞–∑ –≤ –≤—ã–ø–ª–∞—Ç–µ", "—Å—Ç—Ä–∞—Ö–æ–≤–æ–π —Å–ø–æ—Ä",
    "–Ω–µ–¥–æ–ø–ª–∞—Ç–∞", "–¥–æ–ø–ª–∞—Ç–∞", "–ø–µ—Ä–µ—Å—á–µ—Ç", "–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞",
    "–Ω–∞—Ä—É—à–µ–Ω–∏–µ —Å—Ä–æ–∫–æ–≤", "–∑–∞—Ç—è–≥–∏–≤–∞–Ω–∏–µ –≤—ã–ø–ª–∞—Ç", "–Ω–µ–¥–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ—Å—Ç—å"
]

EXCLUDED_KEYWORDS = [
    "—Ä–µ–∫–ª–∞–º–∞", "–ø—Ä–æ–¥–∞—é", "–∫—É–ø–∏—Ç—å", "—Å–∫–∏–¥–∫–∞", "–∞–∫—Ü–∏—è", "–ø—Ä–æ–º–æ–∫–æ–¥",
    "–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏–π", "—Å–ø–æ–Ω—Å–æ—Ä", "—Ä–µ–∫–ª–∞–º–∞", "bitcoin", "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞"
]

# –ö—ç—à –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ —Ö—ç—à—É –∑–∞–≥–æ–ª–æ–≤–∫–∞)
PROCESSED_NEWS = set()

log = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _total_reactions(msg: types.Message) -> int:
    if msg.reactions and isinstance(msg.reactions, types.MessageReactions):
        return sum(r.count for r in msg.reactions.results)
    return 0


def _is_relevant_content(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞."""
    if not text:
        return False

    text_lower = text.lower()

    # –ò—Å–∫–ª—é—á–∞–µ–º —Ä–µ–∫–ª–∞–º—É
    if any(keyword in text_lower for keyword in EXCLUDED_KEYWORDS):
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    relevant_score = sum(
        1 for keyword in NEWS_KEYWORDS if keyword in text_lower)

    # –ö–æ–Ω—Ç–µ–Ω—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º 2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞ (–ø–æ–≤—ã—Å–∏–ª–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
    # –ò–õ–ò –µ—Å—Ç—å 1 –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ –æ—Å–æ–±–æ –≤–∞–∂–Ω—ã—Ö
    high_priority_keywords = ["–û–°–ê–ì–û", "–ö–ê–°–ö–û", "–î–¢–ü",
                              "—Å—Ç—Ä–∞—Ö–æ–≤–∞—è –≤—ã–ø–ª–∞—Ç–∞", "–≤–æ–∑–º–µ—â–µ–Ω–∏–µ —É—â–µ—Ä–±–∞", "–†–°–ê"]
    has_priority = any(
        keyword in text_lower for keyword in high_priority_keywords)

    return relevant_score >= 2 or (relevant_score >= 1 and has_priority)


def _extract_key_facts(text: str) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ AI."""
    if not text:
        return ""

    # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–µ–≥–æ
    lines = [line.strip() for line in text.split('\n') if line.strip()]

    # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –∏ —Ö—ç—à—Ç–µ–≥–∏ –≤ –∫–æ–Ω—Ü–µ
    clean_lines = []
    for line in lines:
        if line.startswith(('http', '@', '#')) or '–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å' in line.lower():
            break
        clean_lines.append(line)

    return '\n'.join(clean_lines[:5])  # –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫


# ---------------------------------------------------------------------------
# Jobs
# ---------------------------------------------------------------------------

async def collect_subscribers_job(ctx: ContextTypes.DEFAULT_TYPE):
    """Upsert subscribers of target channel once per day."""
    telethon_client: TelegramClient = ctx.bot_data.get("telethon")
    session_maker: async_sessionmaker = ctx.bot_data["db_sessionmaker"]
    channel_id = ctx.bot_data.get("TARGET_CHANNEL_ID")
    if not telethon_client or not channel_id:
        return

    async for participant in telethon_client.iter_participants(channel_id):
        async with session_maker() as session:
            await session.merge(Subscriber(
                id=participant.id,
                username=participant.username,
                first_name=participant.first_name,
                last_name=participant.last_name,
                is_bot=participant.bot or False,
                last_seen=datetime.utcnow(),
            ))
            await session.commit()


async def scan_external_channels_job(ctx: ContextTypes.DEFAULT_TYPE):
    """Fetch recent posts from external channels and store popular ones."""
    if not EXTERNAL_CHANNELS:
        log.warning(
            "scan_external: EXTERNAL_CHANNELS is empty; set env var EXTERNAL_CHANNELS=vc_ru,rbc_news,‚Ä¶")
        return
    telethon_client: TelegramClient = ctx.bot_data.get("telethon")
    session_maker: async_sessionmaker = ctx.bot_data["db_sessionmaker"]
    if not telethon_client:
        return

    for channel in EXTERNAL_CHANNELS:
        log.info("scan_external: fetching %s", channel)
        try:
            messages = await telethon_client.get_messages(channel, limit=50)
        except Exception as e:
            log.warning("failed to fetch %s: %s", channel, e)
            continue

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏ –ò —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        popular = [m for m in messages if (
            (m.views or 0) >= VIEWS_THRESHOLD or _total_reactions(
                m) >= REACTIONS_THRESHOLD
        ) and _is_relevant_content(m.message)]

        log.info("scan_external: found %d popular + relevant messages from %s",
                 len(popular), channel)
        saved = 0
        async with session_maker() as session:
            for m in popular:
                exists = await session.scalar(select(ExternalPost.id).where(
                    ExternalPost.channel == channel,
                    ExternalPost.message_id == m.id
                ))
                if exists:
                    continue
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ AI
                clean_text = _extract_key_facts(m.message or "")
                post = ExternalPost(
                    channel=channel,
                    message_id=m.id,
                    date=m.date or datetime.utcnow(),
                    views=m.views,
                    reactions=_total_reactions(m),
                    text=clean_text,
                )
                session.add(post)
                saved += 1
            await session.commit()
        if saved:
            log.info("scan_external: saved %s new posts from %s", saved, channel)
        else:
            log.info("scan_external: no new popular posts from %s", channel)


# ---------------------------------------------------------------------------
# RSS Parsing Functions
# ---------------------------------------------------------------------------

async def fetch_rss_feed(url: str, timeout: int = 10) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç RSS –ª–µ–Ω—Ç—É –ø–æ URL."""
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=timeout)) as session:
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.text()
    except Exception as e:
        log.warning("RSS fetch failed for %s: %s", url, e)
    return None


def parse_rss_items(rss_content: str) -> List[Dict[str, str]]:
    """–ü–∞—Ä—Å–∏—Ç RSS XML –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π."""
    items = []
    try:
        root = ET.fromstring(rss_content)

        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ RSS
        for item in root.findall(".//item"):
            title = item.find("title")
            description = item.find("description")
            link = item.find("link")
            pub_date = item.find("pubDate")

            if title is not None and title.text:
                # –û—á–∏—Å—Ç–∫–∞ HTML —Ç–µ–≥–æ–≤ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                desc_text = ""
                if description is not None and description.text:
                    desc_text = re.sub(
                        r'<[^>]+>', '', description.text).strip()

                items.append({
                    "title": title.text.strip(),
                    "description": desc_text,
                    "link": link.text.strip() if link is not None and link.text else "",
                    "pub_date": pub_date.text.strip() if pub_date is not None and pub_date.text else ""
                })
    except Exception as e:
        log.error("RSS parsing failed: %s", e)

    return items


def get_content_hash(title: str, description: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ö—ç—à –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏."""
    content = f"{title}{description}".lower()
    return hashlib.md5(content.encode()).hexdigest()


async def scan_rss_sources_job(ctx: ContextTypes.DEFAULT_TYPE):
    """–°–∫–∞–Ω–∏—Ä—É–µ—Ç RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏."""
    session_maker: async_sessionmaker = ctx.bot_data["db_sessionmaker"]

    total_sources = len(RSS_SOURCES)
    total_processed = 0
    total_relevant = 0
    total_saved = 0

    for i, source in enumerate(RSS_SOURCES, 1):
        log.info("scan_rss: [%d/%d] fetching %s (%s)",
                 i, total_sources, source["name"], source["category"])

        try:
            rss_content = await fetch_rss_feed(source["url"])
            if not rss_content:
                log.warning(
                    "scan_rss: failed to fetch %s - RSS unavailable", source["name"])
                continue

            items = parse_rss_items(rss_content)
            total_processed += len(items)
            log.info("scan_rss: parsed %d items from %s",
                     len(items), source["name"])

            relevant_items = []
            for item in items:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                full_text = f"{item['title']} {item['description']}"
                if _is_relevant_content(full_text):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
                    content_hash = get_content_hash(
                        item['title'], item['description'])
                    if content_hash not in PROCESSED_NEWS:
                        relevant_items.append((item, content_hash))
                        PROCESSED_NEWS.add(content_hash)

            total_relevant += len(relevant_items)
            log.info("scan_rss: found %d relevant + unique items from %s (%.1f%% relevance)",
                     len(relevant_items), source["name"],
                     (len(relevant_items) / len(items) * 100) if items else 0)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
            saved = 0
            async with session_maker() as session:
                for item, content_hash in relevant_items:
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö—ç—à –∫–∞–∫ message_id –¥–ª—è RSS –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                        exists = await session.scalar(select(ExternalPost.id).where(
                            ExternalPost.channel == source["name"],
                            # –ü–µ—Ä–≤—ã–µ 8 —Å–∏–º–≤–æ–ª–æ–≤ —Ö—ç—à–∞ –∫–∞–∫ int
                            ExternalPost.message_id == int(
                                content_hash[:8], 16)
                        ))
                        if exists:
                            continue

                        # –ì–æ—Ç–æ–≤–∏–º —Å–∂–∞—Ç—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è AI
                        clean_text = f"{item['title']}\n\n{_extract_key_facts(item['description'])}"

                        post = ExternalPost(
                            channel=source["name"],
                            message_id=int(content_hash[:8], 16),
                            date=datetime.utcnow(),
                            views=500,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π "—Ä–µ–π—Ç–∏–Ω–≥" –¥–ª—è RSS
                            reactions=10,
                            text=clean_text,
                        )
                        session.add(post)
                        saved += 1
                    except Exception as e:
                        log.warning(
                            "scan_rss: failed to save item from %s: %s", source["name"], e)

                await session.commit()

            total_saved += saved
            if saved:
                log.info("scan_rss: ‚úÖ saved %d new posts from %s",
                         saved, source["name"])
            elif relevant_items:
                log.info("scan_rss: üìù %d items from %s already exist",
                         len(relevant_items), source["name"])
            else:
                log.info("scan_rss: ‚ùå no relevant content from %s",
                         source["name"])

        except Exception as e:
            log.error("scan_rss: ‚ö†Ô∏è error processing %s: %s",
                      source["name"], e)

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    log.info("scan_rss: SUMMARY - processed %d items from %d sources, found %d relevant, saved %d new posts",
             total_processed, total_sources, total_relevant, total_saved)


# ---------------------------------------------------------------------------
# Posting job
# ---------------------------------------------------------------------------


async def post_from_external_job(ctx: ContextTypes.DEFAULT_TYPE):
    """Pick one unsent external post, rewrite via AI, publish, mark as posted."""
    telethon_client = ctx.bot  # not used but keep signature
    session_maker: async_sessionmaker = ctx.bot_data["db_sessionmaker"]
    channel_id = ctx.bot_data.get("TARGET_CHANNEL_ID")
    if not channel_id:
        return

    async with session_maker() as session:
        post: ExternalPost | None = await session.scalar(
            select(ExternalPost).where(ExternalPost.posted.is_(False)
                                       ).order_by(ExternalPost.views.desc()).limit(1)
        )
        if not post:
            log.info("post_external: no unposted items found")
            return

        # AI rewrite
        site_brief = (
            "–¢—ã –∫–æ–ø–∏—Ä–∞–π—Ç–µ—Ä –∫–∞–Ω–∞–ª–∞ '–°—Ç—Ä–∞—Ö–æ–≤–∞—è —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç—å'. –ü–µ—Ä–µ–ø–∏—à–∏ –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è –Ω–∞—à–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ —Å—Ç—Ä–∞—Ö–æ–≤—ã—Ö –≤—ã–ø–ª–∞—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è —Ñ–∞–∫—Ç—ã, –¥–æ–±–∞–≤—å –æ–¥–∏–Ω –≤—ã–≤–æ–¥, –Ω–æ —É–±–µ—Ä–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤. 400-500 —Å–∏–º–≤–æ–ª–æ–≤, –º–∞–∫—Å–∏–º—É–º –¥–≤–µ —ç–º–æ–¥–∑–∏."
        )
        from bot_final import _ai_complete, humanize

        messages = [
            {"role": "system", "content": site_brief},
            {"role": "user", "content": post.text or ""},
        ]
        text = await _ai_complete(messages, temperature=0.7, max_tokens=600)
        if text:
            text = await humanize(text)
        else:
            text = post.text or ""

        from bot_final import send_text_only
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç –±–µ–∑ –º–µ–¥–∏–∞
        bot = ctx.bot
        await send_text_only(bot, channel_id, text, reply_markup=None)
        log.info("post_external: posted message %s from %s views=%s",
                 post.message_id, post.channel, post.views)

        post.posted = True
        await session.commit()


async def get_rss_stats_job(ctx: ContextTypes.DEFAULT_TYPE):
    """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º."""
    session_maker: async_sessionmaker = ctx.bot_data["db_sessionmaker"]

    try:
        async with session_maker() as session:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
            from sqlalchemy import func, text

            result = await session.execute(text("""
                SELECT 
                    channel,
                    COUNT(*) as total_posts,
                    COUNT(CASE WHEN posted = true THEN 1 END) as posted_count,
                    AVG(views) as avg_views,
                    MAX(date) as last_post_date
                FROM external_posts 
                WHERE channel LIKE '%rss%' OR channel LIKE '%news%' 
                GROUP BY channel 
                ORDER BY total_posts DESC
            """))

            stats = result.fetchall()

            log.info("=== RSS SOURCES STATISTICS ===")
            for stat in stats:
                channel, total, posted, avg_views, last_date = stat
                usage_rate = (posted / total * 100) if total > 0 else 0
                log.info("üìä %s: %d posts, %d used (%.1f%%), avg_views: %.0f, last: %s",
                         channel, total, posted, usage_rate, avg_views or 0,
                         last_date.strftime("%Y-%m-%d") if last_date else "N/A")

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_external = sum(stat[1] for stat in stats)
            total_used = sum(stat[2] for stat in stats)
            overall_rate = (total_used / total_external *
                            100) if total_external > 0 else 0

            log.info("=== OVERALL RSS STATS ===")
            log.info("üìà Total external posts: %d, Used: %d (%.1f%% usage rate)",
                     total_external, total_used, overall_rate)

    except Exception as e:
        log.error("get_rss_stats_job failed: %s", e)
